{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EbfhRLv0zejb"
   },
   "source": [
    "\n",
    "## Graph Autoencoder based Collaborative Filtering\n",
    "#Update 2020.03.21\n",
    "\n",
    " - As the model is so big we need to save and reload the last save epoch with checkpoint.\n",
    "\n",
    "#Update 2020.03.14\n",
    "- Deep and wide neighours n_order, k_neighbour -> $n^k$ inputs\n",
    "- **Note** the model consumes lots of RAM with deeper and wider nodes\n",
    "\n",
    "Main settings in 4A: \n",
    "\n",
    "#Update 2020.03.02\n",
    "- Integrated validation set during training\n",
    "- Integrated early stopping with delta = 1e-5\n",
    "- Use 'adadelta' optimizer for dynamic learning rate\n",
    "- user n neibor + item n neibor\n",
    "- @base: 3/03/20 discussion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wuBt2WWN5Zrf"
   },
   "source": [
    "#[New Model](https://drive.google.com/file/d/1kN5loA18WyF1-I7BskOw6c9P1bdArxk7/view?usp=sharing): \n",
    "\n",
    "![Click](https://drive.google.com/file/d/1kN5loA18WyF1-I7BskOw6c9P1bdArxk7/view?usp=sharing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mWjyzcXW54GG"
   },
   "source": [
    "#Model implementation framework\n",
    "\n",
    "TF2.0 and Keras implementation\n",
    "\n",
    "- Create GMF model\n",
    "    - Create helper methods: User/item latent\n",
    "    - Create loss functions\n",
    "    - Handle input $u_i, v_j$\n",
    "    - Handle output $\\hat{r}_{ij}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "koO06XoHRo_K"
   },
   "source": [
    "## Organise imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PhlM3OtBzRdr",
    "outputId": "6f071d79-7345-4007-b12e-7310c2c8e103"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "#import\n",
    "#tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Embedding, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import dot, add\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XFEsPmNydxl3"
   },
   "outputs": [],
   "source": [
    "#dt_dir_name= \"C:/Users/jiyu/Desktop/Mo/sample_data/ml-1m\"\n",
    "dt_dir_name= \"C:/Users/thinguyen/Desktop/PhD_2020/Python Code/GNN/Mo/sample_data/Amazon_Book_small\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file saved_models_WiHi_MLP(2,1) already exists.\n",
      "Error occurred while processing: saved_models_WiHi_MLP(2,1).\n"
     ]
    }
   ],
   "source": [
    "#prepare folder structures\n",
    "\n",
    "saved_model_dir = 'saved_models_WiHi_MLP(2,1)/'\n",
    "!mkdir -p \"saved_models_WiHi_MLP(2,1)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_saved_model = True\n",
    "n_order = 2\n",
    "k_neighbour=1\n",
    "lr = 0.0005\n",
    "l1_reg=1e-5\n",
    "l2_reg=1e-4\n",
    "k=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "Yvt1j3H7M_Yl",
    "outputId": "dc9b894a-6a60-4503-c202-b4d687acc5ce"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(dt_dir_name +'/'+ 'ratings.csv', names=['user_id', 'item_id', 'rating'])\n",
    "#dataset = pd.read_csv(dt_dir_name +'/'+ \"ratings.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y50GEUeWrgYL"
   },
   "outputs": [],
   "source": [
    "#reindex from 0 ids\n",
    "dataset.user_id = dataset.user_id.astype('category').cat.codes.values\n",
    "dataset.item_id = dataset.item_id.astype('category').cat.codes.values\n",
    "#createMFModel(dataset=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3JkJvoIbS4gd"
   },
   "source": [
    "##Turn original dataset to negative sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HYxI9uKCQ9Gl"
   },
   "outputs": [],
   "source": [
    "#Version 1.2 (flexible + superfast negative sampling uniform)\n",
    "import random\n",
    "import time\n",
    "import scipy\n",
    "\n",
    "def neg_sampling(ratings_df, n_neg=1, neg_val=0, pos_val=1, percent_print=5):\n",
    "  \"\"\"version 1.2: 1 positive 1 neg (2 times bigger than the original dataset by default)\n",
    "\n",
    "    Parameters:\n",
    "    input rating data as pandas dataframe: userId|movieId|rating\n",
    "    n_neg: include n_negative / 1 positive\n",
    "\n",
    "    Returns:\n",
    "    negative sampled set as pandas dataframe\n",
    "            userId|movieId|interact (implicit)\n",
    "  \"\"\"\n",
    "  sparse_mat = scipy.sparse.coo_matrix((ratings_df.rating, (ratings_df.user_id, ratings_df.item_id)))\n",
    "  dense_mat = np.asarray(sparse_mat.todense())\n",
    "  print(dense_mat.shape)\n",
    "\n",
    "  nsamples = ratings_df[['user_id', 'item_id']]\n",
    "  nsamples['rating'] = nsamples.apply(lambda row: 1, axis=1)\n",
    "  length = dense_mat.shape[0]\n",
    "  printpc = int(length * percent_print/100)\n",
    "\n",
    "  nTempData = []\n",
    "  i = 0\n",
    "  start_time = time.time()\n",
    "  stop_time = time.time()\n",
    "\n",
    "  extra_samples = 0\n",
    "  for row in dense_mat:\n",
    "    if(i%printpc==0):\n",
    "      stop_time = time.time()\n",
    "      print(\"processed ... {0:0.2f}% ...{1:0.2f}secs\".format(float(i)*100 / length, stop_time - start_time))\n",
    "      start_time = stop_time\n",
    "\n",
    "    n_non_0 = len(np.nonzero(row)[0])\n",
    "    zero_indices = np.where(row==0)[0]\n",
    "    if(n_non_0 * n_neg + extra_samples >= len(zero_indices)):\n",
    "      print(i, \"non 0:\", n_non_0,\": len \",len(zero_indices))\n",
    "      neg_indices = zero_indices.tolist()\n",
    "      extra_samples = n_non_0 * n_neg + extra_samples - len(zero_indices)\n",
    "    else:\n",
    "      neg_indices = random.sample(zero_indices.tolist(), n_non_0 * n_neg + extra_samples)\n",
    "      extra_samples = 0\n",
    "\n",
    "    nTempData.extend([(uu, ii, rr) for (uu, ii, rr) in zip(np.repeat(i, len(neg_indices))\n",
    "                    , neg_indices, np.repeat(neg_val, len(neg_indices)))])\n",
    "    i+=1\n",
    "\n",
    "  nsamples=nsamples.append(pd.DataFrame(nTempData, columns=[\"user_id\",\"item_id\", \"rating\"]),ignore_index=True)\n",
    "  nsamples.reset_index(drop=True)\n",
    "  return nsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "y_14eDLzQ5tY",
    "outputId": "b1ec141a-6269-4f74-98cc-fe6f35983a48"
   },
   "outputs": [],
   "source": [
    "neg_dataset = neg_sampling(dataset, n_neg=1)\n",
    "neg_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "utsDgdnjiKGe"
   },
   "source": [
    "##Create train test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bXY34jFnUd8A"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(neg_dataset, test_size=0.2, random_state=2020)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gYNfcOkbFaxL"
   },
   "source": [
    "#Create deep embedding using MLP of the [model](https://drive.google.com/file/d/1kN5loA18WyF1-I7BskOw6c9P1bdArxk7/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yd2F19dTFmpi"
   },
   "outputs": [],
   "source": [
    "uids = np.sort(dataset.user_id.unique())\n",
    "iids = np.sort(dataset.item_id.unique())\n",
    "\n",
    "n_users = len(uids)\n",
    "n_items = len(iids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XrPNkCqOsY3h",
    "outputId": "8de02878-dc86-436b-a6a6-9c323d34d641"
   },
   "outputs": [],
   "source": [
    "n_users, n_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mUH0ZY-U9GUa"
   },
   "source": [
    "## Create deep autoencoder (Skipped this)\n",
    "\n",
    "\n",
    "Reference: [keras](https://blog.keras.io/building-autoencoders-in-keras.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qFc7u4Y0kk0o"
   },
   "source": [
    "#Create rating matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TYBlPffk_4jG"
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "sparse_mat = scipy.sparse.coo_matrix((neg_dataset.rating, (neg_dataset.user_id, neg_dataset.item_id)))\n",
    "rating_matrix = np.asarray(sparse_mat.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "zY_2RWV4AK-y",
    "outputId": "34fc95f8-4188-4de5-fd90-90a8622b202d"
   },
   "outputs": [],
   "source": [
    "rating_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T7owpsQpJBER"
   },
   "source": [
    "#Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f5Gbtsl1JEGV"
   },
   "outputs": [],
   "source": [
    "def create_hidden_size(n_hidden_layers = 3, n_latent_factors = 8):\n",
    "  \"\"\"Sizes of each hidden layer, decreasing order\"\"\"\n",
    "  hidden_size = [n_latent_factors*2**i for i in reversed(range(n_hidden_layers))]\n",
    "  return hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9HhzvVBi6ouk",
    "outputId": "31ac443e-0376-47e7-ab6e-f64c7fc4a889"
   },
   "outputs": [],
   "source": [
    "create_hidden_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a1mD8CqzznZx"
   },
   "source": [
    "### Create nearest neighbour (using cosine similarity)\n",
    "\n",
    "Deep and wide version! n order + k neighbour \n",
    "Total: $k + k^2 + ...+ k^n$\n",
    "This is fuking insane!\n",
    "- Order 2: first $k$ rows\n",
    "- Order 3: next $k^2$ rows\n",
    "- Order 4: next $k^3$ rows\n",
    "\n",
    "Important pattern when parsing data:\n",
    "\n",
    "\n",
    "$[order 2 \\rightarrow order 3 \\rightarrow order 4]$\n",
    "\n",
    "samples:\n",
    "\n",
    "$[k \\rightarrow k^2 \\rightarrow k^3 ]$\n",
    "\n",
    "**Note**: don't care about loop (self-loop) e.g. $\\Delta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NNtj5B8mNkls"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def create_closest_neighbour_list(rating_matrix, n_order, k_neighbour):\n",
    "  \"\"\"return index list of most (k) similar rows that sorted descendingly of 2, 3,..n order\n",
    "  \n",
    "  Params:\n",
    "     n_order: 1 -> its self, 2-> depth = 2 (include 1 further)\n",
    "     k_neighour: number of neighour nodes each node in each order from 1 -> n.\n",
    "\n",
    "  \"\"\"\n",
    "  k_nb = []\n",
    "  idx = 0\n",
    "  cos_matrix = cosine_similarity(rating_matrix, rating_matrix)\n",
    "  #print(cos_matrix)\n",
    "  for row in cos_matrix:\n",
    "    k_largest = np.argsort(-row)[:k_neighbour+1]\n",
    "    k_largest = k_largest.tolist()\n",
    "    if idx in k_largest:\n",
    "      k_largest.remove(idx)\n",
    "    k_nb.append(k_largest[:k_neighbour])\n",
    "    idx += 1\n",
    "  k_nb_2nd = np.stack(k_nb, axis=1)\n",
    "  #print(k_nb_2nd)\n",
    "\n",
    "  temp = k_nb_2nd\n",
    "  for o in range(2, n_order):\n",
    "    start_idx = sum([k_neighbour*k_neighbour**i for i in range(o-2)])\n",
    "\n",
    "    #print([k_neigbour*k_neigbour**i for i in range(o-2)],\"start:\", start_idx)\n",
    "    temp1 = np.concatenate([np.asarray([k_nb_2nd[:, k] for k in row]).T for row in temp[start_idx:,:]])\n",
    "    temp = np.concatenate([temp,temp1])\n",
    "\n",
    "  return temp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hpl15LyQlZ9F"
   },
   "source": [
    "#Create model with Keras with shared autoencoder layers\n",
    "\n",
    "Reference: shared vision model: https://keras.io/getting-started/functional-api-guide/#shared-vision-model\n",
    "\n",
    "Problem: graph disconnect : https://github.com/keras-team/keras/issues/11151\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b8OhJEI-TOCn"
   },
   "source": [
    "###Create custom loss for ui,um,& items\n",
    "\n",
    "Currently not in use!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mtoxMSlrtQWK"
   },
   "source": [
    "###Create shared autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4CnewmMGvgUG"
   },
   "outputs": [],
   "source": [
    "def createSharedAutoEncoder(input_shape, hidden_size, names=['user_encoder', 'user_decoder']):\n",
    "    \"\"\"This method is to create autoencoder\n",
    "\n",
    "    Parameters: \n",
    "    input_shape: tuble for shape. For this method, one value is expected, e.g. (30, ).\n",
    "    hidden_size: the array that contains number of neuron each layers, e.g. [10, 20, 1]\n",
    "  \n",
    "    Returns: \n",
    "    encoder: the encoder model\n",
    "    decoder: the decoder model\n",
    "    \"\"\"\n",
    "    # shared autoencoder\n",
    "    input=Input(shape=input_shape)\n",
    "    encoded = input\n",
    "    for nn in hidden_size[:-1]:\n",
    "        encoded = Dense(nn, activation='relu',kernel_initializer='he_uniform')(encoded) \n",
    "    encoded = Dense(hidden_size[-1], activation='relu',kernel_initializer='he_uniform',\n",
    "                  name=names[0])(encoded) \n",
    "    encoder = Model(input, encoded, name=names[0])\n",
    "\n",
    "    #------- decoder model\n",
    "    hidden_size.reverse()\n",
    "    decoderinput = Input(shape=(hidden_size[0]))\n",
    "    decoded = decoderinput\n",
    "    for nn in hidden_size[1:]:\n",
    "      decoded = Dense(nn, activation='relu', kernel_initializer='he_uniform')(decoded)\n",
    "    decoded = Dense(input_shape[0], activation='relu', kernel_initializer='he_uniform', name=names[1])(decoded)\n",
    "    decoder = Model(decoderinput, decoded, name=names[1])\n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tpN5OKg-vRvM"
   },
   "source": [
    "###Integrate autoencoders + mlp + custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ZgDmpzeE9lV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_input_weights(n_order, k_neighbour, decay=4):\n",
    "  layer_weights = [np.repeat(decay**(n_order-o-1), k_neighbour**o) for o in range(n_order)]\n",
    "  layer_weights_flat = np.concatenate(layer_weights).ravel()\n",
    "  layer_weights_sum = np.sum(layer_weights_flat)\n",
    "  layer_weights_normalized = layer_weights_flat / layer_weights_sum\n",
    "  return layer_weights_normalized\n",
    "\n",
    "get_input_weights(2, 1, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qKSGawhd1nqS"
   },
   "outputs": [],
   "source": [
    "def create_model(n_users, n_items, n_order=2, k_neighbour=1, latent_factors=64, lr = 0.0005, l1_reg=1e-5, l2_reg=1e-4):\n",
    "  \"\"\"\n",
    "  number of depth = n_order, n_order=2: 1 node + 1 deeper node\n",
    "  \"\"\"\n",
    "\n",
    "  #user shared autoencoder\n",
    "  hidden_size = create_hidden_size() #for autoencoder\n",
    "  uencoder, udecoder = createSharedAutoEncoder((n_items,), hidden_size)\n",
    "  #item shared autoencoder\n",
    "  hidden_size = create_hidden_size() #for autoencoder\n",
    "  iencoder, idecoder = createSharedAutoEncoder((n_users,), \n",
    "                                               hidden_size,['item_encoder','item_decoder'])   \n",
    "\n",
    "  #create n inputs + shared autoencoder\n",
    "  u_inputs = []\n",
    "  v_inputs = []\n",
    "\n",
    "  u_encoded = []\n",
    "  v_encoded = []\n",
    "  u_decoded = []\n",
    "  v_decoded = []\n",
    "  #n-order proximity by comparing n embedded vecs\n",
    "  input_weights = get_input_weights(n_order, k_neighbour, decay=4)\n",
    "\n",
    "  for i in range(n_order):\n",
    "    u_inputs.extend([Input(shape=(n_items,), name= f'ui{i}{k}') for k in range(k_neighbour**i)])\n",
    "    v_inputs.extend([Input(shape=(n_users,), name= f'vj{i}{k}') for k in range(k_neighbour**i)])\n",
    "\n",
    "  u_encoded.extend([uencoder(u_i) for u_i in u_inputs])\n",
    "  v_encoded.extend([iencoder(v_j) for v_j in v_inputs])\n",
    "  \n",
    "  u_decoded.extend([udecoder(u_en) for u_en in u_encoded])\n",
    "  v_decoded.extend([idecoder(v_en) for v_en in v_encoded])\n",
    "\n",
    "  #get ALL COMBINED embeddings from 2 encoders(Need work with combining method)\n",
    "  uii_encoded = add([u_encoded[i]*input_weights[i] for i in range(len(u_encoded))]) if n_order > 1 and k_neighbour > 0 else u_encoded[0]\n",
    "  vji_encoded = add([v_encoded[i]*input_weights[i] for i in range(len(u_encoded))]) if n_order > 1 and k_neighbour > 0 else v_encoded[0]\n",
    "\n",
    "  concat = layers.concatenate([uii_encoded, vji_encoded])\n",
    "  mlp = concat\n",
    "  for i in range(3,-1,-1):\n",
    "    if i == 0:\n",
    "      mlp = Dense(8**i, activation='sigmoid', name=\"mlp\")(mlp)\n",
    "    else:\n",
    "      mlp = Dense(8*2**i, activation='sigmoid')(mlp)\n",
    "      if i >= 2:\n",
    "        mlp = BatchNormalization()(mlp)\n",
    "        mlp = Dropout(0.2)(mlp)\n",
    "\n",
    "  model = Model(inputs=[u_inputs, v_inputs], \n",
    "                outputs=[u_decoded, v_decoded, mlp])\n",
    "  \n",
    "  udecoder_names=[\"user_decoder\" if x==0 else f\"user_decoder_{x}\" for x in range(len(input_weights))]\n",
    "  vdecoder_names=[\"item_decoder\" if x==0 else f\"item_decoder_{x}\" for x in range(len(input_weights))]\n",
    "\n",
    "  udecoder_dict = {ukey: 'mean_squared_error' for ukey in udecoder_names}\n",
    "  vdecoder_dict = {vkey: 'mean_squared_error' for vkey in vdecoder_names}\n",
    "\n",
    "  udecoder_metric_dict = {ukey: 'mse' for ukey in udecoder_names}\n",
    "  vdecoder_metric_dict = {vkey: 'mse' for vkey in udecoder_names}\n",
    "  \n",
    "\n",
    "  losses={'mlp':'binary_crossentropy', **udecoder_dict, **vdecoder_dict}\n",
    "  \n",
    "  metrics={'mlp':['binary_accuracy'\n",
    "                        ], \n",
    "              **udecoder_metric_dict,\n",
    "              **vdecoder_metric_dict\n",
    "           }\n",
    "  adadelta=tf.keras.optimizers.Adadelta(learning_rate=lr)\n",
    "  model.compile(optimizer='adadelta', loss=losses, metrics=metrics)\n",
    "\n",
    "  model.summary()\n",
    "\n",
    "  return  model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5nYDqsVrtX6o"
   },
   "source": [
    "##Argparse\n",
    "\n",
    "Store all settings here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Rckg9TjWdeHm",
    "outputId": "95c25ee6-cebe-4dfa-f9c6-af1c1c27f86b"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "if load_saved_model:\n",
    "  saved_list = os.listdir(saved_model_dir)\n",
    "  saved_list.sort()\n",
    "  print(saved_list)\n",
    "  if(len(saved_list) != 0): \n",
    "    last_saved = saved_list[-1]\n",
    "    model = tf.keras.models.load_model(saved_model_dir+'/'+last_saved)\n",
    "  else:\n",
    "    model = create_model(n_users, n_items, n_order, k_neighbour)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hL6lccOaleLN"
   },
   "source": [
    "###Create data generator using rating matrix\n",
    "\n",
    "It takes rating matrix and generate a sequence of users, items, and ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dyMtoZLy6SxZ",
    "outputId": "ca1937c8-735d-446a-a156-c495eebe95c7"
   },
   "outputs": [],
   "source": [
    "closest_uneighbor = create_closest_neighbour_list(rating_matrix, n_order, k_neighbour)\n",
    "closest_ineighbor = create_closest_neighbour_list(rating_matrix.T, n_order,k_neighbour)\n",
    "closest_uneighbor.shape, closest_ineighbor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rzlkixAH9q9F"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "import math\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, dataset, rating_matrix, batch_size=100, n_order = 2, k_neighbour=1, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.n_order = n_order\n",
    "        self.batch_size = batch_size\n",
    "        self.dataset = dataset\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = self.dataset.index\n",
    "        self.rating_matrix = rating_matrix\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return math.floor(len(self.dataset) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        idxs = [i for i in range(index*self.batch_size,(index+1)*self.batch_size)]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.indices[k] for k in idxs]\n",
    "\n",
    "        # Generate data\n",
    "        uids = self.dataset.iloc[list_IDs_temp,[0]].to_numpy().reshape(-1)\n",
    "        iids = self.dataset.iloc[list_IDs_temp,[1]].to_numpy().reshape(-1)\n",
    "\n",
    "        Users = np.stack([rating_matrix[row] for row in uids])\n",
    "        Items = np.stack([rating_matrix[:, col] for col in iids])\n",
    "        ratings = self.dataset.iloc[list_IDs_temp,[2]].to_numpy().reshape(-1)\n",
    "\n",
    "        if n_order > 1 and k_neighbour > 0:\n",
    "          u_neighbors = [closest_uneighbor[:,index] for index in uids ]\n",
    "          i_neighbors = [closest_ineighbor[:,index] for index in iids]\n",
    "          #print([np.stack([rating_matrix[row] for row in u_neighbors[i]]) for i in range(len(u_neighbors))])\n",
    "\n",
    "          User_neighbors =list(zip(*[[rating_matrix[rowId] for rowId in u_neighbors[i]] for i in range(len(u_neighbors))]))\n",
    "          #print([u for u in User_neighbors])#, User_neighbors.shape)\n",
    "          User_neighbors = np.array([np.stack(batch) for batch in User_neighbors])\n",
    "\n",
    "          Item_neighbors =list(zip(*[[rating_matrix[:,colId] for colId in i_neighbors[i]] for i in range(len(i_neighbors))]))\n",
    "          Item_neighbors = np.array([np.stack(batch) for batch in Item_neighbors])\n",
    "\n",
    "          return [Users, *User_neighbors, Items, *Item_neighbors],[Users,*User_neighbors, Items, *Item_neighbors, ratings]\n",
    "        else:\n",
    "          return [Users, Items],[Users, Items, ratings]\n",
    "     \n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indices = np.arange(len(self.dataset))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XW6ZseFXRQzV"
   },
   "source": [
    "##Training with data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "63qB2z8jzPKt"
   },
   "outputs": [],
   "source": [
    "#early_stop = EarlyStopping(monitor='val_mlp_loss', min_delta = 0.0001, patience=10)\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "#                               patience=10, min_lr=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Va1XeZWzkBKl"
   },
   "outputs": [],
   "source": [
    "checkpoint_path= saved_model_dir + \"/model-{epoch:02d}-{mlp_binary_accuracy:.2f}.hdf5\"\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, monitor='mlp_binary_accuracy',verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0-yRouiTlUaA"
   },
   "outputs": [],
   "source": [
    "train_generator = DataGenerator(train, rating_matrix, batch_size=256, n_order=n_order, k_neighbour=k_neighbour, shuffle=False)\n",
    "#val_generator = DataGenerator(val, rating_matrix, batch_size=512, n_order=n_order, k_neighbour=k_neighbour,  shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "eWPaVhX251-0",
    "outputId": "80125e7b-6ca2-47cb-dea1-439f0f2c4aba"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_generator,\n",
    "                    # validation_data=val_generator,\n",
    "                    epochs=100, \n",
    "                    verbose=2, callbacks=[cp_callback,\n",
    "                                          # early_stop\n",
    "                                          ],\n",
    "                    #workers=4,\n",
    "                    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2E8-5W-Vis_e"
   },
   "source": [
    "## Plot losses\n",
    "\n",
    "There are several losses, pick the one we need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pyd1JY_tYilg"
   },
   "source": [
    "Let's now see how our model does! I'll do a small post-processing step to round off our prediction to the nearest integer. This is usually not done, and thus just a whimsical step, since the training ratings are all integers! There are better ways to encode this intger requirement (one-hot encoding!), but we won't discuss them in this post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4iQQp_-5Yg8E"
   },
   "outputs": [],
   "source": [
    "test_datagenerator = DataGenerator(test, rating_matrix)\n",
    "\n",
    "results = model.evaluate(test_datagenerator)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#Cal HR according to NCF\n",
    "#Create user & item list:\n",
    "tmp_lst_u=train.user_id.unique()\n",
    "#tmp_lst_i=train.item_id.unique()\n",
    "tmp_lst_i=dataset.item_id.unique()\n",
    "tmp_lst_u.sort(), tmp_lst_i.sort()\n",
    "lst_user=tmp_lst_u.tolist()\n",
    "lst_item=tmp_lst_i.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Top_100_Unused_item(user_id):\n",
    "    tmp_df_used_item=train.loc[(train['user_id']==user_id) & (train['rating']==1)]\n",
    "    tmp_lst=tmp_df_used_item['item_id'].values.tolist()\n",
    "    #lst_un_item= set(lst_item) - set(tmp_lst)\n",
    "    lst_un_item=[x for x in lst_item if x not in tmp_lst]\n",
    "    \n",
    "    #random 100 items:\n",
    "    tmp_no=100000\n",
    "    np.random.seed(2020)\n",
    "    lst_100_un_item=(np.random.choice(lst_un_item,tmp_no))\n",
    "    \n",
    "    \n",
    "    #Create DataFrame\n",
    "    \n",
    "    tmp_df=pd.DataFrame(columns=['user_id', 'item_id', 'rating', 'prediction'])\n",
    "    tmp_df['item_id']=lst_100_un_item\n",
    "    tmp_df['user_id']=user_id\n",
    "    tmp_df['rating']=0.0\n",
    "\n",
    " \n",
    "    top_datagenerator = DataGenerator(tmp_df, rating_matrix)\n",
    "    tmp_y_hat = model.predict(top_datagenerator)\n",
    "    y_hat= tmp_y_hat[4]\n",
    "    tmp_arr=y_hat.flatten().tolist()\n",
    "    tmp_df['prediction']=tmp_arr\n",
    "    return tmp_df\n",
    "    \n",
    "    \n",
    "#tạo item_id array for each user:\n",
    "def recommend(df,u,k):\n",
    "    tmp_df=df.sort_values(by=['prediction'],ascending=False)\n",
    "    tmp_df=tmp_df.head(k)\n",
    "    \n",
    "    \n",
    "    #reset index sẽ dễ cho việc .iloc hoặc .loc\n",
    "    tmp_df.reset_index(drop=True, inplace=True) \n",
    "    \n",
    "    tmp_arrItem=tmp_df['item_id'].to_numpy()\n",
    "    return (tmp_arrItem,tmp_df)\n",
    "\n",
    "def dcg_at_k(r, k):\n",
    "    assert k >= 1\n",
    "    r = np.asfarray(r)[:k] != 0\n",
    "    if r.size:\n",
    "        return np.sum(np.subtract(np.power(2, r), 1) / np.log2(np.arange(2, r.size + 2)))\n",
    "    return 0.\n",
    "\n",
    "def ndcg_at_k(r, k):\n",
    "    assert k >= 1\n",
    "    idcg = dcg_at_k(sorted(r, reverse=True), k)\n",
    "    if not idcg:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k) / idcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "test_2=test.copy()\n",
    "test_2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "k=20\n",
    "rd_no =10\n",
    "np.random.seed(2020)\n",
    "rd_lst_usr=np.random.choice(lst_user,rd_no)\n",
    "#rd_lst_usr=lst_user\n",
    "#________________________________________________________________________________________________\n",
    "#tạo dataframe HR\n",
    "df_HR=pd.DataFrame(columns=['user_id', 'HR','NDCG'])\n",
    "df_HR['user_id']=rd_lst_usr\n",
    "df_HR=df_HR.sort_values(by=['user_id'],ascending=True)\n",
    "\n",
    "for u in rd_lst_usr:\n",
    "    df_100_Unused=Top_100_Unused_item(u)\n",
    "    #get top 20 prediction:\n",
    "    arr_top_k,df_top_k=recommend(df_100_Unused,u,k)\n",
    "    \n",
    "    #Check_with_TestData(df_top_k,test_2)\n",
    "    for i in range(len(df_top_k)):\n",
    "    #Column sort: \"user_id -> item_id -> rating -> prediction\n",
    "        usr=df_top_k.iloc[i,0]\n",
    "        itm=df_top_k.iloc[i,1]\n",
    "        #check xem có row nào trong test_2 thỏa mãn ko, nếu có sẽ tạo ra 1 df có >=1 row\n",
    "        chk=len(test_2.loc[(test_2[\"user_id\"]==usr) & (test_2[\"item_id\"]==itm) & (test_2[\"rating\"]==1)])\n",
    "        if chk==1: \n",
    "            df_top_k.loc[(df_top_k[\"user_id\"]==usr) & (df_top_k[\"item_id\"]==itm),\"rating\"]=1\n",
    "        \n",
    "    rating_lst=df_top_k['rating'].tolist()   \n",
    "    #################################################\n",
    "    #Tính HR:\n",
    "    tmp_cnt=0\n",
    "    for r in rating_lst:\n",
    "        if r!=0:\n",
    "            tmp_cnt += 1\n",
    "    tmp_hr = tmp_cnt/len(rating_lst)\n",
    "    df_HR.loc[df_HR[\"user_id\"]==int(u),[\"HR\"]]=tmp_hr\n",
    "       \n",
    "    ##########################################################\n",
    "    #Tính NDCG:\n",
    "    \n",
    "    ndcg=ndcg_at_k(rating_lst, k)    \n",
    "    df_HR.loc[df_HR[\"user_id\"]==int(u),[\"NDCG\"]]=ndcg\n",
    "#print(df_HR)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate HR and NDCG for the model\n",
    "HR_temp= df_HR.sum(0)\n",
    "\n",
    "HR=HR_temp[1]/(len(df_HR))\n",
    "\n",
    "NDCG=HR_temp[2]/(len(df_HR))\n",
    "print(\"HR= \", HR)\n",
    "print(\"NDCG= \", NDCG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GsdDXeO8Ry7_"
   },
   "source": [
    "#References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tKqSn4KnL2yQ"
   },
   "source": [
    "Input layer:\n",
    "\n",
    "- Embedding layer: [Link](https://gdcoder.com/-what-is-an-embedding-layer/)\n",
    "- Embedding lookup: [link text](https://keras.io/layers/embeddings/)\n",
    "- Multi input: [link text](https://keras.io/getting-started/functional-api-guide/#multi-input-and-multi-output-models)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Model 4b implicit NG: Autoencoders + n-ordered* K_neighbours + MLP",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
